{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob, random, os, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faeac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(r'C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\cassava-leaf-disease-classification\\train_images\\6103.jpg')\n",
    "fig = plt.figure()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to imagenet size \n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0) # add batch dim\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16 # 16 pixels\n",
    "pathes = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "# print(pathes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "Data_path=r'C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\cassava-leaf-disease-classification'\n",
    "training_images=r'C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\cassava-leaf-disease-classification\\train_images'\n",
    "training_path=r'C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\cassava-leaf-disease-classification\\train.csv'\n",
    "testing_path=r'C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\cassava-leaf-disease-classification\\test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let look at the training data\n",
    "wholeData=os.path.join(Data_path, \"train.csv\")\n",
    "wholeData=pd.read_csv(wholeData)\n",
    "wholeData.head()\n",
    "\n",
    "# stratify ensures that the proportion of different classes in the original dataset is maintained in both the training and testing subsets.\n",
    "training_data, valid_data=train_test_split(wholeData, test_size=0.2, random_state=42, stratify=wholeData.label.values) \n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "wholeData.label.value_counts().plot(kind='bar', color='blue', position=0, width=0.20, label='Whole Data')\n",
    "training_data.label.value_counts().plot(kind='bar', color='orange', position=1, width=0.20, label='Training Data')\n",
    "valid_data.label.value_counts().plot(kind='bar', color='green', position=2, width=0.20, label='Validation Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, wholeData, data_path=Data_path, transform=None, mode=\"train\"):\n",
    "        super().__init__()\n",
    "        self.data=wholeData.values  #Covert df into 2D array\n",
    "        self.data_path=data_path\n",
    "        self.transform=transform\n",
    "        self.mode=mode\n",
    "        self.data_dir= \"train_images\" if mode==\"train\" else \"test_images\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label=self.data[index]\n",
    "        image=os.path.join(self.data_path, self.data_dir, image);\n",
    "        OpenImage=Image.open(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            OpenImage=self.transform(OpenImage)\n",
    "        \n",
    "        return OpenImage, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74202b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "from torchvision.transforms import transforms\n",
    "transformsTrain=transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(100),\n",
    "        transforms.RandomResizedCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformsValid = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eee4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= CassavaDataset(training_data, transform=transformsTrain) #train_dataset is a tensor array that contain all the training images at index 0 and its label at index 1 a/q to index\n",
    "valid_dataset = CassavaDataset(valid_data, transform=transformsValid)\n",
    "print(train_dataset[14][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f068a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=48,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=48,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023863f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1) #concatenation\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x\n",
    "    \n",
    "PatchEmbedding()(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5023979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "MultiHeadAttention()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaaeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3828eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 5):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c74cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 5,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d70016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ViT().to(device)\n",
    "print(model(x.to(device)))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234417c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=48\n",
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    train_acc = 0.\n",
    "    \n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        inputs, labels = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        train_acc += torch.sum(outputs.argmax(dim=1).to(device) == labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 20 == 0:\n",
    "            last_loss = running_loss / 20 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "#             tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            running_loss = 0.\n",
    "\n",
    "    return train_acc/((len(train_loader)*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Epoch: {}'.format(i + 1))\n",
    "    out=train_one_epoch(i)\n",
    "    print(\"Final:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e91ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_check():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    train_acc = 0.\n",
    "    \n",
    "    for i, (data, target) in enumerate(valid_loader):\n",
    "        inputs, labels = data.to(device), target.to(device)\n",
    "        outputs = model(inputs)\n",
    "        temp_acc = torch.sum(outputs.argmax(dim=1).to(device) == labels)\n",
    "        train_acc+=temp_acc\n",
    "        if i%10==0:\n",
    "            print('  batch {} correct: {}'.format(i + 1, (train_acc/(batch_size*(i+1)))))\n",
    "\n",
    "    return train_acc/(len(valid_loader)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Accuracy:\", valid_check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving\n",
    "epoch=5\n",
    "PATH = r\"C:\\Users\\MOJAHID HUSSAIN\\Desktop\\vit\\model_checkpoint.pth\"\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ViT()\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "criterion = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
